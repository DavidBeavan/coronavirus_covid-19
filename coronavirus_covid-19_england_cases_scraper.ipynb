{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coronavirus (COVID-19) England Cases Scraper\n",
    "## David Beavan @DavidBeavan\n",
    "## Licence: MIT. Sources: see below\n",
    "## Notes\n",
    "* This is liable to break, lots if the official sources change\n",
    "* One problem is that the offical csv data has no date info\n",
    "* We try to fudge it when scraping today and the data looks like yesterday we do not add it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data\n",
    "# Data is not packaged with code, Will download from sources and save for future runsdata_base_dir = Path('data/secondary_sources')\n",
    "data_base_dir = Path('data/secondary_sources')\n",
    "data_base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_sub_dir = data_base_dir.joinpath('utla_cases_table')\n",
    "data_sub_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "utla_cases_file = data_sub_dir.joinpath('utla_cases.csv')\n",
    "\n",
    "# Load existing data, if present, otherwise start with a blank dataframe\n",
    "if utla_cases_file.exists():\n",
    "    utla_cases_df = pd.read_csv(utla_cases_file, index_col='utla')\n",
    "else:\n",
    "    utla_cases_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_data(df, existing_df=None):\n",
    "    # Take date of new column\n",
    "    scrape_date = list(df)[0]\n",
    "\n",
    "    # If we have an existing dataframe we are adding this column to\n",
    "    if existing_df is not None and len(existing_df) > 0:\n",
    "        # If the new data is the same date as existing data\n",
    "        # replace it if different, but if the same to nothing\n",
    "\n",
    "        if scrape_date in existing_df:\n",
    "            if not existing_df[scrape_date].equals(df[scrape_date]):\n",
    "                existing_df[scrape_date] = df[scrape_date]\n",
    "                df = existing_df\n",
    "            else:\n",
    "                df = existing_df\n",
    "        else:\n",
    "            # We do not have this new date in the existing dataframe\n",
    "            # See if this new data matches yesterday's\n",
    "            # This is the case if we scrape early in a day and assume the date is today\n",
    "            # This is all thanks to assuming the data is dated when it is scraped\n",
    "            # We do this because there is no date info in the primary source\n",
    "\n",
    "            scrape_date_object = datetime.date.fromisoformat(scrape_date)\n",
    "            scrape_date_object = scrape_date_object - \\\n",
    "                datetime.timedelta(days=1)\n",
    "            scrape_date_yesterday = scrape_date_object.isoformat()\n",
    "\n",
    "            # If the new data is the same data as yesterday do nothing\n",
    "            # Otherwise add the new data column\n",
    "            if not (scrape_date_yesterday in existing_df.columns and existing_df[scrape_date_yesterday].equals(df[scrape_date])):\n",
    "                existing_df = existing_df.join(df, how='outer')\n",
    "                df = existing_df\n",
    "            else:\n",
    "                df = existing_df\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retired as data now comes as csv\n",
    "\n",
    "# def scrape_page(url, existing_df=None):\n",
    "\n",
    "#     # Fetch url, it is a web page\n",
    "#     r = requests.get(url)\n",
    "#     soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "#     # Find paragraph that matches the date of the data\n",
    "#     date_paras = soup.find_all('p', text=re.compile('These data are as of'))\n",
    "#     if len(date_paras) != 1:\n",
    "#         print('Error: too many possible dates')\n",
    "\n",
    "#     date_para = date_paras[0].text\n",
    "\n",
    "#     # Find the date, parse it and keep the iso data format as text\n",
    "#     date_match = re.search('These data are as of .+? on (.+)\\.', date_para)\n",
    "#     date_string = date_match.group(1)\n",
    "#     datetime_object = datetime.datetime.strptime(date_string, '%d  %B %Y')\n",
    "#     scrape_date = datetime_object.date().isoformat()\n",
    "\n",
    "#     # Find the data table and make a dataframe from it\n",
    "#     table = soup.find_all('table')[0]\n",
    "#     df = pd.read_html(str(table), index_col=0)\n",
    "#     df = df[0]\n",
    "\n",
    "#     # Neaten up the dataframe\n",
    "#     df.index.rename('utla', inplace=True)\n",
    "#     df.columns = [scrape_date]\n",
    "\n",
    "#     # Remove those with no location, later datasets do not give awaiting info\n",
    "#     df.drop('Awaiting confirmation', inplace=True, errors='ignore')\n",
    "\n",
    "#     # As of 2020-03-09 The following areas were merged, we will retrospectivaly do this for early data\n",
    "#     df.rename(index={'Cornwall': 'Cornwall and Isles of Scilly',\n",
    "#                      'Hackney': 'Hackney and City of London'}, inplace=True)\n",
    "\n",
    "#     # Add this column to the existing dataframe\n",
    "#     df = add_new_data(df, existing_df)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_csv(url, existing_df=None, override_date=None):\n",
    "\n",
    "    # Fetch url, it is a csv\n",
    "    df = pd.read_csv(url, index_col='GSS_CD')\n",
    "    df.index.name = 'utla'\n",
    "\n",
    "    # Drop unneeded columns\n",
    "    df.drop(columns=['GSS_NM'], inplace=True)\n",
    "\n",
    "    # Set date of data, overriding if needed\n",
    "    if override_date is not None:\n",
    "        df.columns = [override_date]\n",
    "    else:\n",
    "        scrape_date = datetime.date.today().isoformat()\n",
    "        df.columns = [scrape_date]\n",
    "\n",
    "    # Add this column to the existing dataframe\n",
    "    df = add_new_data(df, existing_df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retired as we have transitioned from text indexes to utla codes when data source moved to csv\n",
    "\n",
    "# def reindex(df):\n",
    "#     # Fetch url of most recent data, it is a csv\n",
    "#     data_df = pd.read_csv(\n",
    "#         'https://www.arcgis.com/sharing/rest/content/items/b684319181f94875a6879bbc833ca3a6/data')\n",
    "\n",
    "#     # Select code and name colmuns\n",
    "#     data_df = data_df[['GSS_CD', 'GSS_NM']]\n",
    "\n",
    "#     # Index on name, to match the dataframe\n",
    "#     data_df.set_index('GSS_NM', inplace=True)\n",
    "#     data_df.index.name = 'utla'\n",
    "\n",
    "#     # Join to dataframe, so we now have code accessable\n",
    "#     df = df.join(data_df)\n",
    "\n",
    "#     # Change index to code\n",
    "#     df.set_index('GSS_CD', inplace=True)\n",
    "#     df.index.name = 'utla'\n",
    "\n",
    "#     # Add in names in index 0\n",
    "#     df.insert(0, 'GSS_NM', data_df.index.tolist())\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Retired as data now comes as csv\n",
    "\n",
    "# 8 March 2020\n",
    "# utla_cases_df = scrape_page('https://web.archive.org/web/20200308150435/https://www.gov.uk/government/publications/coronavirus-covid-19-number-of-cases-in-england/coronavirus-covid-19-number-of-cases-in-england', utla_cases_df)\n",
    "\n",
    "# 9 March 2020\n",
    "# utla_cases_df = scrape_page('https://web.archive.org/web/20200309190503/https://www.gov.uk/government/publications/coronavirus-covid-19-number-of-cases-in-england/coronavirus-covid-19-number-of-cases-in-england', utla_cases_df)\n",
    "\n",
    "# 10 March 2020\n",
    "# utla_cases_df = scrape_page('https://web.archive.org/web/20200310222310/https://www.gov.uk/government/publications/coronavirus-covid-19-number-of-cases-in-england/coronavirus-covid-19-number-of-cases-in-england', utla_cases_df)\n",
    "\n",
    "# 11 March 2020\n",
    "# utla_cases_df = scrape_page('https://web.archive.org/web/20200311173829/https://www.gov.uk/government/publications/coronavirus-covid-19-number-of-cases-in-england/coronavirus-covid-19-number-of-cases-in-england', utla_cases_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More recent case data, not archived, replaced daily, but then that is what we are doing\n",
    "\n",
    "# utla_cases_df = scrape_csv('https://www.arcgis.com/sharing/rest/content/items/b684319181f94875a6879bbc833ca3a6/data', utla_cases_df, '2020-03-14')\n",
    "utla_cases_df = scrape_csv(\n",
    "    'https://www.arcgis.com/sharing/rest/content/items/b684319181f94875a6879bbc833ca3a6/data', utla_cases_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GSS_NM</th>\n",
       "      <th>2020-03-08</th>\n",
       "      <th>2020-03-09</th>\n",
       "      <th>2020-03-10</th>\n",
       "      <th>2020-03-11</th>\n",
       "      <th>2020-03-12</th>\n",
       "      <th>2020-03-13</th>\n",
       "      <th>2020-03-14</th>\n",
       "      <th>2020-03-15</th>\n",
       "      <th>2020-03-16</th>\n",
       "      <th>2020-03-17</th>\n",
       "      <th>2020-03-18</th>\n",
       "      <th>2020-03-19</th>\n",
       "      <th>2020-03-20</th>\n",
       "      <th>2020-03-21</th>\n",
       "      <th>2020-03-22</th>\n",
       "      <th>2020-03-23</th>\n",
       "      <th>2020-03-24</th>\n",
       "      <th>2020-03-25</th>\n",
       "      <th>2020-03-26</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utla</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E09000002</th>\n",
       "      <td>Barking and Dagenham</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>42</td>\n",
       "      <td>45</td>\n",
       "      <td>53</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E09000003</th>\n",
       "      <td>Barnet</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>76</td>\n",
       "      <td>81</td>\n",
       "      <td>88</td>\n",
       "      <td>94</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E08000016</th>\n",
       "      <td>Barnsley</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06000022</th>\n",
       "      <td>Bath and North East Somerset</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E06000055</th>\n",
       "      <td>Bedford</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 GSS_NM  2020-03-08  2020-03-09  2020-03-10  \\\n",
       "utla                                                                          \n",
       "E09000002          Barking and Dagenham           0           0           1   \n",
       "E09000003                        Barnet           4           5           8   \n",
       "E08000016                      Barnsley           2           2           2   \n",
       "E06000022  Bath and North East Somerset           0           0           0   \n",
       "E06000055                       Bedford           0           0           0   \n",
       "\n",
       "           2020-03-11  2020-03-12  2020-03-13  2020-03-14  2020-03-15  \\\n",
       "utla                                                                    \n",
       "E09000002           1           3           4           5           7   \n",
       "E09000003           8           9          14          23          25   \n",
       "E08000016           2           2           2           3           5   \n",
       "E06000022           0           0           0           1           2   \n",
       "E06000055           0           0           0           0           0   \n",
       "\n",
       "           2020-03-16  2020-03-17  2020-03-18  2020-03-19  2020-03-20  \\\n",
       "utla                                                                    \n",
       "E09000002          10           9          14          18          21   \n",
       "E09000003          25          24          27          28          76   \n",
       "E08000016           4           2           6           7           7   \n",
       "E06000022           1           3           3           4           5   \n",
       "E06000055           0           1           1           1           1   \n",
       "\n",
       "           2020-03-21  2020-03-22  2020-03-23  2020-03-24  2020-03-25  \\\n",
       "utla                                                                    \n",
       "E09000002          30          35          42          45          53   \n",
       "E09000003          81          88          94          99         100   \n",
       "E08000016           7          10          11          15          19   \n",
       "E06000022          11          13          16          17          20   \n",
       "E06000055           4           4           7           7           7   \n",
       "\n",
       "           2020-03-26  \n",
       "utla                   \n",
       "E09000002          72  \n",
       "E09000003         103  \n",
       "E08000016          27  \n",
       "E06000022          23  \n",
       "E06000055          10  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview data\n",
    "utla_cases_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GSS_NM        Barking and DagenhamBarnetBarnsleyBath and Nor...\n",
       "2020-03-08                                                  224\n",
       "2020-03-09                                                  253\n",
       "2020-03-10                                                  309\n",
       "2020-03-11                                                  357\n",
       "2020-03-12                                                  434\n",
       "2020-03-13                                                  535\n",
       "2020-03-14                                                  764\n",
       "2020-03-15                                                  975\n",
       "2020-03-16                                                 1109\n",
       "2020-03-17                                                 1421\n",
       "2020-03-18                                                 2065\n",
       "2020-03-19                                                 2544\n",
       "2020-03-20                                                 3246\n",
       "2020-03-21                                                 3995\n",
       "2020-03-22                                                 4623\n",
       "2020-03-23                                                 5402\n",
       "2020-03-24                                                 6606\n",
       "2020-03-25                                                 7697\n",
       "2020-03-26                                                 9324\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview data\n",
    "utla_cases_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/secondary_sources_bak/secondary_sources_bak_2020-03-26')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save data\n",
    "utla_cases_df.to_csv(utla_cases_file)\n",
    "\n",
    "# Delete old backup\n",
    "backup_dir = Path('data/secondary_sources_bak/secondary_sources_bak_' +\n",
    "                  datetime.date.today().isoformat())\n",
    "if backup_dir.exists():\n",
    "    shutil.rmtree(backup_dir)\n",
    "\n",
    "# Make new backup\n",
    "shutil.copytree(Path('data/secondary_sources'), backup_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
